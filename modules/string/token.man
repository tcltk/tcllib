[manpage_begin string::token n 1.1]
[keywords lexing]
[keywords regex]
[keywords string]
[keywords tokenization]
[moddesc   {Text and string utilities}]
[titledesc {Regex based iterative lexing}]
[category  {Text processing}]
[require Tcl "8.5 9"]
[require string::token [opt 1.1]]
[require fileutil]
[description]

This package provides commands for regular expression based lexing
(tokenizing) of strings.

[para]

[section API]

[list_begin definitions]

[comment {- - -- --- ----- -------- ------------- ---------------------}]
[call [cmd {::string token text}] [arg lex] [arg string]]

This command takes a dictionary, [arg lex], which must map regular
expressions (“regexps”) to labels, and tokenizes the [arg string]
according to the dictionary.

[para]
The command tests each regexp in the [arg lex] dictionary in order.
Since Tcl [type dict]s are insertion-ordered, it is recommended that
the regexps are specified from most specific to most general.

[para]
The command returns a list of tokens. Each token is a 3-element list
containing the label (from the [arg lex] [type dict]), and the start and
end indexes of the match in the [arg string].

[para]
The command will throw an error if it is not able to tokenize the entire
string.

[para]
For line-oriented lexers, finer-grained error handling and handling huge
files can be achieved by lexing line-by-line.

[comment {- - -- --- ----- -------- ------------- ---------------------}]
[call [cmd {::string token file}] [arg lex] [arg path]]

This command is a convenience wrapper around [cmd {::string token text}]
that makes it easy to tokenize entire files.

[para]
[emph Note] that this command loads the whole file into memory before
starting to process it.

[comment {- - -- --- ----- -------- ------------- ---------------------}]
[call [cmd {::string token chomp}] [arg lex] [arg startVar] \
[arg string] [arg resultVar]]

[para]
This command is exposed to enable users to write their own lexers. For
example, to apply different lexing dictionaries depending on internal
state as the lex progresses.

[para]
(The [cmd {::string token text}] and [cmd {::string token file}] commands
make use of this command behind the scenes.)

[para]
This command takes a dictionary, [arg lex], which must map regexps to
labels, a variable [arg startVar] which contains the index position
where the lexing should begin in the input [arg string], and a result
variable [arg resultVar] to which each successfuly lexed token will be
appended.

[para]
The command returns a status code indicating the state of the lex
These are
[list_begin definitions]
[def [const 0]] No token found.
[def [const 1]] Token found.
[def [const 2]] End of string reached.
[list_end]

[para]
If a token was recognized (status [const 1]) the command will update the
index in [arg startVar] to the character immediately [emph following]
the lexed token. It will also append to the [arg resultVar] the lexed
token which itself is a 3-element list containing the label (from the
[arg lex] [type dict]), and the start and end indexes of the match in
the [arg string].

[para]
If a token was not recognized (status [const 0] or [const 2]),
neither [arg startVar] nor [arg resultVar] will be updated.

[para]
As noted earlier, the command tests each regexp in the [arg lex]
dictionary in order. Since Tcl [type dict]s are insertion-ordered, it is
recommended that the regexps are specified from most specific to most
general.

[para]
Further note that all regex patterns are implicitly prefixed
with the constraint escape [const \\A] to ensure that a match starts
exactly at the character index found in [arg startVar].

[list_end]

[section Examples]

This example shows the basic use of the [cmd {string token text}]
command to lex a simple [emph .ini] file in one go. The [cmd {string
token file}] command works in the same way, except it tokenizes the
contents of a file rather than a string.

[example {
const INI_EG "; example.ini 
Filename = test.db
 
\[Database\]
# remote data
Port = 143
IP = 192.0.2.62
"
 
const INI_LEX [dict create {[#;][^\n]+?\n} COMMENT {\[[^]\n]+\]} GROUP\
    {\w+\s*[:=][^\n]+} ENTRY {\s*\n} BLANK]
foreach token [string token text $INI_LEX $INI_EG] {
    lassign $token label i j
    switch $label {
        BLANK { continue }
        COMMENT {
            puts "Comment: '[string trim [string range $INI_EG $i+1 $j]]'"
        }
        GROUP { puts "Group  : '[string range $INI_EG $i+1 $j-1]'" }
        ENTRY {
            set entry [string range $INI_EG $i $j]
            set k [lindex [regexp -inline -indices {[=:]} $entry] 0 0]
            set key [string trim [string range $entry 0 $k-1]]
            set value [string trim [string range $entry $k+1 end]]
            puts "Entry  : Key='$key' Value='$value'"
        }
    }
}

=>
Comment: 'example.ini'
Entry  : Key='Filename' Value='test.db'
Group  : 'Database'
Comment: 'remote data'
Entry  : Key='Port' Value='143'
Entry  : Key='IP' Value='192.0.2.62'
}]

[vset CATEGORY textutil]
[include ../common-text/feedback.inc]
[manpage_end]
