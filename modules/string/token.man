[manpage_begin string::token n 1.1]
[keywords lexing]
[keywords regex]
[keywords string]
[keywords tokenization]
[moddesc   {Text and string utilities}]
[titledesc {Regex based iterative lexing}]
[category  {Text processing}]
[require Tcl "8.5 9"]
[require string::token [opt 1.1]]
[require fileutil]
[description]

This package provides commands for regular expression based lexing
(tokenizing) of strings.

[para]

[section API]

[list_begin definitions]

[comment {- - -- --- ----- -------- ------------- ---------------------}]
[call [cmd {::string token text}] [arg lex] [arg string]]

This command takes a dictionary, [arg lex], which must map regular
expressions (“regexps”) to labels, and tokenizes the [arg string]
according to the dictionary.

[para]
The command tests each regexp in the [arg lex] dictionary in order.
Since Tcl [type dict]s are insertion-ordered, it is recommended that
the regexps are specified from most specific to most general.

[para]
The command returns a list of tokens. Each token is a 3-element list
containing the label (from the [arg lex] [type dict]), and the start and
end indexes of the match in the [arg string].

[para]
The command will throw an error if it is not able to tokenize the entire
string.

[para]
For line-oriented lexers, finer-grained error handling and handling huge
files can be achieved by lexing line-by-line.

[comment {- - -- --- ----- -------- ------------- ---------------------}]
[call [cmd {::string token file}] [arg lex] [arg path]]

This command is a convenience wrapper around [cmd {::string token text}]
that makes it easy to tokenize entire files.

[para]
[emph Note] that this command loads the whole file into memory before
starting to process it.

[comment {- - -- --- ----- -------- ------------- ---------------------}]
[call [cmd {::string token chomp}] [arg lex] [arg startVar] \
[arg string] [arg resultVar]]

[para]
This command is exposed to enable users to write their own lexers. For
example, to apply different lexing dictionaries depending on internal
state as the lex progresses.

[para]
(The [cmd {::string token text}] and [cmd {::string token file}] commands
make use of this command behind the scenes.)

[para]
This command takes a dictionary, [arg lex], which must map regexps to
labels, a variable [arg startVar] which contains the index position
where the lexing should begin in the input [arg string], and a result
variable [arg resultVar] to which each successfuly lexed token will be
appended.

[para]
The command returns a status code indicating the state of the lex
These are
[list_begin definitions]
[def [const 0]] No token found.
[def [const 1]] Token found.
[def [const 2]] End of string reached.
[list_end]

[para]
If a token was recognized (status [const 1]) the command will update the
index in [arg startVar] to the character immediately [emph following]
the lexed token. It will also append to the [arg resultVar] the lexed
token which itself is a 3-element list containing the label (from the
[arg lex] [type dict]), and the start and end indexes of the match in
the [arg string].

[para]
If a token was not recognized (status [const 0] or [const 2]),
neither [arg startVar] nor [arg resultVar] will be updated.

[para]
As noted earlier, the command tests each regexp in the [arg lex]
dictionary in order. Since Tcl [type dict]s are insertion-ordered, it is
recommended that the regexps are specified from most specific to most
general.

[para]
Further note that all regex patterns are implicitly prefixed
with the constraint escape [const \\A] to ensure that a match starts
exactly at the character index found in [arg startVar].

[list_end]

[section Examples]

This example shows how to use the [cmd {string token text}] command to lex a
simple line-oriented [emph .ini] file format.
(For full-featured [emph .ini] file handling, see
[uri \
https://core.tcl-lang.org/tcllib/doc/trunk/embedded/md/tcllib/files/modules/inifile/ini.md inifile].)

[example {
const INI_EG "; example.ini 
Filename = test.db
 
invalid
\[Database\]
# remote data
Port: 143
IP: 192.0.2.62
 
junk
"

proc main {} {
    set groups [lex_ini $::INI_EG]
    puts General:
    dict for {key value} [dict get $groups General] { puts "\t$key: $value" }
    puts Database:
    dict for {key value} [dict get $groups Database] { puts "\t$key: $value" }
}

proc lex_ini ini_text {
    const INI_LEX [dict create {[#;].*$} COMMENT {\[[^]]+\]} GROUP\
        {\w+\s*[:=].+$} ENTRY]
    set group General
    set linenum 0
    foreach line [split $ini_text \n] {
        incr linenum
        if {[string trim $line] eq ""} { continue }
        try {
            set token [string token text $INI_LEX $line]
            if {[llength $token]} { set token [lindex $token 0] }
            lassign $token label i j
            switch $label {
                COMMENT { continue }
                GROUP { 
                    set group [string range $line $i+1 $j-1]
                    dict set groups $group {}
                }
                ENTRY {
                    set entry [string range $line $i $j]
                    set k [lindex [regexp -inline -indices {[=:]} $entry] 0 0]
                    set key [string trim [string range $entry 0 $k-1]]
                    set value [string trim [string range $entry $k+1 end]]
                    dict set groups $group $key $value
                }
            }
        } on error err {
            puts "Error: line $linenum: $err"
        }
    }
    return $groups
}

main
=>
Error: line 4: Unexpected character 'i' at offset 0
Error: line 10: Unexpected character 'j' at offset 0
General:
	Filename: test.db
Database:
	Port: 143
	IP: 192.0.2.62
}]

[vset CATEGORY textutil]
[include ../common-text/feedback.inc]
[manpage_end]
